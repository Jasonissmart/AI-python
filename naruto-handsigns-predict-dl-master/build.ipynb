{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8160 images belonging to 9 classes.\n",
      "Found 8160 images belonging to 9 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 364.375 steps, validate for 364.375 steps\n",
      "Epoch 1/30\n",
      "365/364 [==============================] - 166s 456ms/step - loss: 3.5311 - accuracy: 0.1767 - val_loss: 2.0232 - val_accuracy: 0.2214\n",
      "Epoch 2/30\n",
      "365/364 [==============================] - 90s 248ms/step - loss: 1.9905 - accuracy: 0.2348 - val_loss: 1.8805 - val_accuracy: 0.3414\n",
      "Epoch 3/30\n",
      "365/364 [==============================] - 79s 216ms/step - loss: 1.8097 - accuracy: 0.3250 - val_loss: 1.6909 - val_accuracy: 0.4010\n",
      "Epoch 4/30\n",
      "365/364 [==============================] - 82s 223ms/step - loss: 1.6970 - accuracy: 0.3640 - val_loss: 1.6182 - val_accuracy: 0.4072\n",
      "Epoch 5/30\n",
      "365/364 [==============================] - 82s 226ms/step - loss: 1.6467 - accuracy: 0.3786 - val_loss: 1.6159 - val_accuracy: 0.3533\n",
      "Epoch 6/30\n",
      "365/364 [==============================] - 81s 222ms/step - loss: 1.5723 - accuracy: 0.4101 - val_loss: 1.7839 - val_accuracy: 0.3714\n",
      "Epoch 7/30\n",
      "365/364 [==============================] - 87s 238ms/step - loss: 1.5228 - accuracy: 0.4298 - val_loss: 1.5958 - val_accuracy: 0.4373\n",
      "Epoch 8/30\n",
      "365/364 [==============================] - 91s 249ms/step - loss: 1.4798 - accuracy: 0.4728 - val_loss: 1.5155 - val_accuracy: 0.4870\n",
      "Epoch 9/30\n",
      "365/364 [==============================] - 88s 242ms/step - loss: 1.4137 - accuracy: 0.4938 - val_loss: 1.6935 - val_accuracy: 0.4998\n",
      "Epoch 10/30\n",
      "365/364 [==============================] - 86s 237ms/step - loss: 1.3839 - accuracy: 0.5134 - val_loss: 1.6416 - val_accuracy: 0.4491\n",
      "Epoch 11/30\n",
      "365/364 [==============================] - 83s 227ms/step - loss: 1.3313 - accuracy: 0.5310 - val_loss: 1.4748 - val_accuracy: 0.5360\n",
      "Epoch 12/30\n",
      "365/364 [==============================] - 82s 226ms/step - loss: 1.3032 - accuracy: 0.5450 - val_loss: 1.4386 - val_accuracy: 0.5313\n",
      "Epoch 13/30\n",
      "365/364 [==============================] - 84s 230ms/step - loss: 1.2505 - accuracy: 0.5574 - val_loss: 1.6566 - val_accuracy: 0.4791\n",
      "Epoch 14/30\n",
      "365/364 [==============================] - 88s 241ms/step - loss: 1.1995 - accuracy: 0.5829 - val_loss: 1.5337 - val_accuracy: 0.5279\n",
      "Epoch 15/30\n",
      "365/364 [==============================] - 81s 222ms/step - loss: 1.2048 - accuracy: 0.5803 - val_loss: 1.4036 - val_accuracy: 0.5375\n",
      "Epoch 16/30\n",
      "365/364 [==============================] - 83s 228ms/step - loss: 1.1094 - accuracy: 0.6103 - val_loss: 1.5333 - val_accuracy: 0.5108\n",
      "Epoch 17/30\n",
      "365/364 [==============================] - 84s 229ms/step - loss: 1.1171 - accuracy: 0.6079 - val_loss: 1.4507 - val_accuracy: 0.5330\n",
      "Epoch 18/30\n",
      "365/364 [==============================] - 73s 200ms/step - loss: 1.0768 - accuracy: 0.6228 - val_loss: 1.4302 - val_accuracy: 0.5663\n",
      "Epoch 19/30\n",
      "365/364 [==============================] - 74s 204ms/step - loss: 1.0352 - accuracy: 0.6344 - val_loss: 1.3261 - val_accuracy: 0.5714\n",
      "Epoch 20/30\n",
      "365/364 [==============================] - 78s 213ms/step - loss: 1.0270 - accuracy: 0.6394 - val_loss: 1.3859 - val_accuracy: 0.5741\n",
      "Epoch 21/30\n",
      "365/364 [==============================] - 77s 212ms/step - loss: 1.0093 - accuracy: 0.6423 - val_loss: 1.3737 - val_accuracy: 0.5776\n",
      "Epoch 22/30\n",
      "365/364 [==============================] - 84s 231ms/step - loss: 0.9663 - accuracy: 0.6545 - val_loss: 1.3400 - val_accuracy: 0.6122\n",
      "Epoch 23/30\n",
      "365/364 [==============================] - 93s 254ms/step - loss: 0.9559 - accuracy: 0.6637 - val_loss: 1.4471 - val_accuracy: 0.5942\n",
      "Epoch 24/30\n",
      "365/364 [==============================] - 82s 224ms/step - loss: 0.9260 - accuracy: 0.6702 - val_loss: 1.6728 - val_accuracy: 0.5880\n",
      "Epoch 25/30\n",
      "365/364 [==============================] - 85s 232ms/step - loss: 0.9261 - accuracy: 0.6724 - val_loss: 1.4962 - val_accuracy: 0.6024\n",
      "Epoch 26/30\n",
      "365/364 [==============================] - 84s 231ms/step - loss: 0.8979 - accuracy: 0.6793 - val_loss: 1.5211 - val_accuracy: 0.5976\n",
      "Epoch 27/30\n",
      "365/364 [==============================] - 103s 283ms/step - loss: 0.9131 - accuracy: 0.6825 - val_loss: 1.4420 - val_accuracy: 0.6043\n",
      "Epoch 28/30\n",
      "365/364 [==============================] - 94s 257ms/step - loss: 0.8936 - accuracy: 0.6866 - val_loss: 1.5169 - val_accuracy: 0.6002\n",
      "Epoch 29/30\n",
      "365/364 [==============================] - 90s 247ms/step - loss: 0.8428 - accuracy: 0.7021 - val_loss: 1.3742 - val_accuracy: 0.6170\n",
      "Epoch 30/30\n",
      "365/364 [==============================] - 78s 213ms/step - loss: 0.9061 - accuracy: 0.6789 - val_loss: 1.2397 - val_accuracy: 0.6286\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator         #To increase the number of data\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "import json\n",
    "\n",
    "image_width, image_height = 64, 64\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, image_width, image_height)\n",
    "else:\n",
    "    input_shape = (image_width, image_height, 1)\n",
    "\n",
    "train_dir = r'C:\\Users\\User\\Desktop\\程式\\AI-python\\naruto-handsigns-predict-dl-master\\datasetTrain'                #input-to train\n",
    "validation_dir = r'C:\\Users\\User\\Desktop\\程式\\AI-python\\naruto-handsigns-predict-dl-master\\datasetTest'    #input-to test\n",
    "\n",
    "save_dir=r'C:\\Users\\User\\Desktop\\程式\\AI-python\\naruto-handsigns-predict-dl-master\\save'      #Data generated by ImageDataGenerator\n",
    "\n",
    "train_samples = 5830               #input\n",
    "validation_samples = 5830           #input\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "\n",
    "#Model Create\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(9))                            #input\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Generate Data\n",
    "#參考資料: https://zhuanlan.zhihu.com/p/30197320\n",
    "#shear_range:错切变换, zoom_range:让图片在长或宽的方向进行放大, horizontal_flip:随机对图片执行水平翻转操作\n",
    "train_datagen = ImageDataGenerator(zoom_range=0.0, horizontal_flip=True,vertical_flip=True,rotation_range=30,shear_range=0.5,width_shift_range=0.5,height_shift_range=0.5)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(image_width, image_height), batch_size=batch_size, \n",
    "                                                    color_mode=\"grayscale\", class_mode='categorical',save_to_dir=save_dir)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(shear_range=0.1, zoom_range=0.5)\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(image_width, image_height), batch_size=batch_size, \n",
    "                                                        color_mode=\"grayscale\", class_mode='categorical',save_to_dir=save_dir)\n",
    "\n",
    "# steps_per_epoch設置說明: https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/\n",
    "model.fit_generator(train_generator, steps_per_epoch= train_samples / batch_size, epochs=epochs, validation_data=validation_generator, \n",
    "                    validation_steps= validation_samples / batch_size)\n",
    "\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model_in_json.json\", \"w\") as json_file:\n",
    "    json.dump(model_json, json_file)\n",
    "\n",
    "\n",
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
